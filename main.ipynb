{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4rCKcndPybL"
   },
   "source": [
    "# Lab : Image Classification using Convolutional Neural Networks\n",
    "\n",
    "At the end of this laboratory, you would get familiarized with\n",
    "\n",
    "*   Creating deep networks using Keras\n",
    "*   Steps necessary in training a neural network\n",
    "*   Prediction and performance analysis using neural networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdglSzOi4Cp-"
   },
   "source": [
    "# **In case you use a colaboratory environment**\n",
    "By default, Colab notebooks run on CPU.\n",
    "You can switch your notebook to run with GPU.\n",
    "\n",
    "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
    "\n",
    "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
    "\n",
    "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wkicuxZdrdq"
   },
   "source": [
    "# **Working with a new dataset: CIFAR-10**\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
    "*   Convert the labels to one-hot encoded form.\n",
    "*   Normalize the images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mrb20KGMtTFq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names for CIFAR-10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Task 1: Visualize 10 random samples from each class\n",
    "def plot_samples_per_class(x, y, class_names):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(10):  # For each class\n",
    "        class_idx = np.where(y.flatten() == i)[0]  # Indices of images of this class\n",
    "        random_indices = np.random.choice(class_idx, 10, replace=False)  # Randomly select 10 images\n",
    "        for j, idx in enumerate(random_indices):\n",
    "            plt.subplot(10, 10, i * 10 + j + 1)\n",
    "            plt.imshow(x[idx])\n",
    "            plt.axis('off')\n",
    "            if j == 0:\n",
    "                plt.title(class_names[i], fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_samples_per_class(x_train, y_train, class_names)\n",
    "\n",
    "# Task 2: Convert labels to one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Task 3: Normalize images\n",
    "x_train_normalized = x_train.astype('float32') / 255.0\n",
    "x_test_normalized = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ER5WlMNRydp"
   },
   "source": [
    "## Define the following model (same as the one in tutorial)\n",
    "\n",
    "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer. \n",
    "\n",
    "Use the input as (32,32,3). \n",
    "\n",
    "The filter maps can then be flattened to provide features to the classifier. \n",
    "\n",
    "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfWCHxh8HGhN"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSN6riPISBMG"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    # Convolutional Layer\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    # Max Pooling Layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Flatten Layer\n",
    "    Flatten(),\n",
    "    # Dense Layer with 100 units\n",
    "    Dense(100, activation='relu'),\n",
    "    # Classification Layer with softmax\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGtivbQJT39U"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn8UzPBZugVp"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',   # Loss function\n",
    "    optimizer=SGD(learning_rate=0.01), # Stochastic Gradient Descent optimizer\n",
    "    metrics=['accuracy']              # Metric: accuracy\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train_normalized,               # Normalized training images\n",
    "    y_train_one_hot,                  # One-hot encoded training labels\n",
    "    epochs=50,                        # Number of epochs\n",
    "    batch_size=512,                   # Batch size\n",
    "    validation_data=(x_test_normalized, y_test_one_hot),  # Validation data\n",
    "    verbose=1                         # Print training progress\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(x_test_normalized, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Plot the cross entropy loss curve and the accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract loss and accuracy values from the training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Plot the cross-entropy loss curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(val_loss, label='Validation Loss', color='orange')\n",
    "plt.title('Cross-Entropy Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot the accuracy curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracy, label='Training Accuracy', color='blue')\n",
    "plt.plot(val_accuracy, label='Validation Accuracy', color='orange')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2mrWK5hSB_o"
   },
   "source": [
    "## Defining Deeper Architectures: VGG Models\n",
    "\n",
    "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
    "\n",
    "Stack two convolutional layers with 32 filters, each of 3 x 3. \n",
    "\n",
    "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer. \n",
    "\n",
    "For all the layers, use ReLU activation function. \n",
    "\n",
    "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A80vLxW9FIek"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgca5dUNSFNc"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the deeper VGG-style model\n",
    "model_vgg = Sequential([\n",
    "    # First Convolutional Block: 2 Conv Layers with 32 filters each\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
    "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    \n",
    "    # Max Pooling Layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Flatten the output to feed into a Dense layer\n",
    "    Flatten(),\n",
    "    \n",
    "    # Dense Layer with 128 units\n",
    "    Dense(128, activation='relu'),\n",
    "    \n",
    "    # Output Layer with 10 units for classification (softmax activation)\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display model summary\n",
    "model_vgg.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwaPphEBUtlC"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc2qtU0mUvVA"
   },
   "outputs": [],
   "source": [
    "# Compile the VGG model with specified settings\n",
    "model_vgg.compile(\n",
    "    loss='categorical_crossentropy',   # Loss function for multi-class classification\n",
    "    optimizer=SGD(learning_rate=0.01), # Stochastic Gradient Descent optimizer with learning rate of 0.01\n",
    "    metrics=['accuracy']               # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Train the model for 50 epochs with a batch size of 512\n",
    "history_vgg = model_vgg.fit(\n",
    "    x_train_normalized,                # Normalized CIFAR-10 training images\n",
    "    y_train_one_hot,                   # One-hot encoded training labels\n",
    "    epochs=50,                         # Number of epochs for training\n",
    "    batch_size=512,                    # Batch size of 512\n",
    "    validation_data=(x_test_normalized, y_test_one_hot), # Validation data for evaluation\n",
    "    verbose=1                          # Display training progress\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss_vgg, test_accuracy_vgg = model_vgg.evaluate(x_test_normalized, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss_vgg:.4f}, Test Accuracy: {test_accuracy_vgg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2cRr2ZFSFds"
   },
   "source": [
    "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8OSHAf5SJPr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract history data for both models\n",
    "# For the initial model (assuming it was stored in `history`)\n",
    "train_loss_initial = history.history['loss']\n",
    "val_loss_initial = history.history['val_loss']\n",
    "train_accuracy_initial = history.history['accuracy']\n",
    "val_accuracy_initial = history.history['val_accuracy']\n",
    "\n",
    "# For the VGG model\n",
    "train_loss_vgg = history_vgg.history['loss']\n",
    "val_loss_vgg = history_vgg.history['val_loss']\n",
    "train_accuracy_vgg = history_vgg.history['accuracy']\n",
    "val_accuracy_vgg = history_vgg.history['val_accuracy']\n",
    "\n",
    "# Create the plot for loss curves\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_initial, label='Training Loss (Initial Model)', color='blue')\n",
    "plt.plot(val_loss_initial, label='Validation Loss (Initial Model)', color='orange')\n",
    "plt.plot(train_loss_vgg, label='Training Loss (VGG Model)', color='green')\n",
    "plt.plot(val_loss_vgg, label='Validation Loss (VGG Model)', color='red')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Accuracy curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracy_initial, label='Training Accuracy (Initial Model)', color='blue')\n",
    "plt.plot(val_accuracy_initial, label='Validation Accuracy (Initial Model)', color='orange')\n",
    "plt.plot(train_accuracy_vgg, label='Training Accuracy (VGG Model)', color='green')\n",
    "plt.plot(val_accuracy_vgg, label='Validation Accuracy (VGG Model)', color='red')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri9kU3wa3Rei"
   },
   "source": [
    "**Comment on the observation**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzXmO1WoSKMY"
   },
   "source": [
    "*   Use predict function to predict the output for the test split\n",
    "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DObaoxhaSMUg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the VGG model to predict on the test set\n",
    "y_pred_vgg = model_vgg.predict(x_test_normalized)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_labels = np.argmax(y_pred_vgg, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test_one_hot, axis=1), y_pred_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.title('Confusion Matrix for VGG Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUBrvRomU5O_"
   },
   "source": [
    "**Comment here :**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffwVz-FLSNG7"
   },
   "source": [
    "*    Print the test accuracy for the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4WX3_uLSN5I"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data and get the loss and accuracy\n",
    "test_loss_vgg, test_accuracy_vgg = model_vgg.evaluate(x_test_normalized, y_test_one_hot, verbose=0)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(f\"Test Accuracy: {test_accuracy_vgg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dySqfA6PVBjQ"
   },
   "source": [
    "## Define the complete VGG architecture.\n",
    "\n",
    "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer. \n",
    "\n",
    "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling. \n",
    "\n",
    "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer. \n",
    "\n",
    "For all the layers, use ReLU activation function. \n",
    "\n",
    "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
    "\n",
    "*   Change the size of input to 64 x 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zm35siILFNT0"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH4lDVBuVA_Q"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Define the VGG-style model\n",
    "model_vgg_complete = Sequential([\n",
    "    # First Block: 2 Conv layers with 64 filters, followed by max pooling\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Second Block: 2 Conv layers with 128 filters, followed by max pooling\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Third Block: 2 Conv layers with 256 filters, followed by max pooling\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten the output to feed into dense layers\n",
    "    Flatten(),\n",
    "    \n",
    "    # Dense Layer with 128 units\n",
    "    Dense(128, activation='relu'),\n",
    "    \n",
    "    # Output Layer with 10 units for classification (softmax activation)\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Display model summary\n",
    "model_vgg_complete.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu_B8kJGWhcM"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
    "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4elnDWnjEbmO"
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_vgg_complete.compile(\n",
    "    loss='categorical_crossentropy',   # Loss function for multi-class classification\n",
    "    optimizer=SGD(learning_rate=0.01), # SGD optimizer with learning rate of 0.01\n",
    "    metrics=['accuracy']               # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Train the model for 10 epochs with a batch size of 512\n",
    "history_vgg_complete = model_vgg_complete.fit(\n",
    "    x_train_resized,                # Resized CIFAR-10 training images\n",
    "    y_train_one_hot,                # One-hot encoded labels for training\n",
    "    epochs=10,                      # Train for 10 epochs\n",
    "    batch_size=512,                 # Batch size of 512\n",
    "    validation_data=(x_test_resized, y_test_one_hot), # Validation data\n",
    "    verbose=1                       # Display training progress\n",
    ")\n",
    "\n",
    "# Predict the outputs for the test set\n",
    "y_pred_vgg_complete = model_vgg_complete.predict(x_test_resized)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_labels = np.argmax(y_pred_vgg_complete, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test_one_hot, axis=1), y_pred_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "plt.title('Confusion Matrix for VGG Complete Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Print the test accuracy\n",
    "test_loss_vgg_complete, test_accuracy_vgg_complete = model_vgg_complete.evaluate(x_test_resized, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy_vgg_complete:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dlzFt0SXGDQ"
   },
   "source": [
    "# Understanding deep networks\n",
    "\n",
    "*   What is the use of activation functions in network? Why is it needed?\n",
    "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
    "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPy_1EWXX6fp"
   },
   "source": [
    "**Write the answers below :**\n",
    "\n",
    "1 - Use of activation functions:\n",
    "\n",
    "\n",
    "\n",
    "_Activation functions introduce non-linearity in deep networks, enabling them to learn complex patterns and relationships in the data. They ensure that the network can approximate non-linear functions, which is crucial for solving real-world problems. Activation functions also help in gradient flow during backpropagation, making training more effective. Without them, neural networks would behave like linear models, limiting their learning capacity.\n",
    "\n",
    "2 - Key Differences between sigmoid and softmax:\n",
    "\n",
    "\n",
    "\n",
    "_The sigmoid activation function outputs values between 0 and 1 for each input independently, making it suitable for binary classification. Softmax, on the other hand, outputs a probability distribution across multiple classes, ensuring that the sum of probabilities equals 1. Sigmoid is applied to individual neurons, while softmax considers the relationships between neurons in the output layer. Softmax is used for multi-class classification, whereas sigmoid is used in binary or multi-label classification.\n",
    "\n",
    "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
    "\n",
    "\n",
    "_Categorical crossentropy is used for multi-class classification, where the output is a probability distribution across multiple classes, and only one class label is correct per sample. Binary crossentropy is used for binary classification or multi-label problems, where each output neuron independently predicts a binary value (0 or 1). Categorical crossentropy compares the predicted probability distribution with the true one-hot encoded labels. Binary crossentropy compares each predicted probability with its true binary label (0 or 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
