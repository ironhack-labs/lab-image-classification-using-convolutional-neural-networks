{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tony-labs/lab-image-classification-using-convolutional-neural-networks/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4rCKcndPybL"
      },
      "source": [
        "# Lab : Image Classification using Convolutional Neural Networks\n",
        "\n",
        "At the end of this laboratory, you would get familiarized with\n",
        "\n",
        "*   Creating deep networks using Keras\n",
        "*   Steps necessary in training a neural network\n",
        "*   Prediction and performance analysis using neural networks\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdglSzOi4Cp-"
      },
      "source": [
        "# **In case you use a colaboratory environment**\n",
        "By default, Colab notebooks run on CPU.\n",
        "You can switch your notebook to run with GPU.\n",
        "\n",
        "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
        "\n",
        "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
        "\n",
        "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wkicuxZdrdq"
      },
      "source": [
        "# **Working with a new dataset: CIFAR-10**\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
        "*   Convert the labels to one-hot encoded form.\n",
        "*   Normalize the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrb20KGMtTFq",
        "outputId": "071b4e29-4185-4115-f51e-002dcb843ec5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNqCWD1W0aHq",
        "outputId": "26f498af-2545-4af2-e22b-00c11894a0f6"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "from tensorflow import keras\n",
        "\n",
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebJynGIV42Z-"
      },
      "outputs": [],
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "aAsrLGfw48ev",
        "outputId": "f87e5da1-8a78-457c-895a-25a726b6504b"
      },
      "outputs": [],
      "source": [
        "# prompt: In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import keras\n",
        "import pickle\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
        "fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        axes[i,j].imshow(x_train[np.random.choice(np.where(y_train == i)[0])])\n",
        "        axes[i,j].axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Convert the labels to one-hot encoded form.\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Normalize the images.\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ER5WlMNRydp"
      },
      "source": [
        "## Define the following model (same as the one in tutorial)\n",
        "\n",
        "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer.\n",
        "\n",
        "Use the input as (32,32,3).\n",
        "\n",
        "The filter maps can then be flattened to provide features to the classifier.\n",
        "\n",
        "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfWCHxh8HGhN"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ApQyfNTo7aZn",
        "outputId": "bd2e360f-f38e-4adb-cc3d-7f46c00a144d"
      },
      "outputs": [],
      "source": [
        "# prompt: For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer.\n",
        "# Use the input as (32,32,3).\n",
        "# The filter maps can then be flattened to provide features to the classifier.\n",
        "# Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation).\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = keras.Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSN6riPISBMG"
      },
      "outputs": [],
      "source": [
        "# prompt: Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPAkT9qL8K1W",
        "outputId": "b15f4787-e57d-4829-9492-5e3688923c91"
      },
      "outputs": [],
      "source": [
        "# prompt: Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512.\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=50, batch_size=512, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtivbQJT39U"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "hn8UzPBZugVp",
        "outputId": "bc79dc55-72a0-4091-e463-1665616c5cf7"
      },
      "outputs": [],
      "source": [
        "# prompt: Plot the cross entropy loss curve and the accuracy curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'history' is the object returned by model.fit\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.legend()\n",
        "plt.title('Cross Entropy Loss Curve')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curve')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH9wXQLr0aHt"
      },
      "source": [
        "*   Plot the cross entropy loss curve and the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "GDKXCm250aHu",
        "outputId": "426a80a3-1569-4622-8dc0-57b424c233f3"
      },
      "outputs": [],
      "source": [
        "# Your code here :\n",
        "# Plot both models' accuracy curves\n",
        "plt.plot(history.history['val_accuracy'], label='Basic Model Validation Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='VGG Model Validation Accuracy')\n",
        "plt.title('Accuracy Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mrWK5hSB_o"
      },
      "source": [
        "## Defining Deeper Architectures: VGG Models\n",
        "\n",
        "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
        "\n",
        "Stack two convolutional layers with 32 filters, each of 3 x 3.\n",
        "\n",
        "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A80vLxW9FIek"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cgca5dUNSFNc",
        "outputId": "ca3e4be2-6b45-4929-c42d-090cd067c4b5"
      },
      "outputs": [],
      "source": [
        "# prompt: Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
        "# Stack two convolutional layers with 32 filters, each of 3 x 3.\n",
        "# Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "# For all the layers, use ReLU activation function.\n",
        "# Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        "\n",
        "clear_session()\n",
        "\n",
        "vgg_model = keras.Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "vgg_model.summary()\n",
        "\n",
        "vgg_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "vgg_history = vgg_model.fit(x_train, y_train, epochs=50, batch_size=512, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaPphEBUtlC"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "Bc2qtU0mUvVA",
        "outputId": "75716db3-f98d-4524-8b22-2fde2c44e2aa"
      },
      "outputs": [],
      "source": [
        "# prompt: Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n",
        "\n",
        "# Plot both models' loss curves\n",
        "plt.plot(history.history['val_loss'], label='Basic Model Validation Loss')\n",
        "plt.plot(vgg_history.history['val_loss'], label='VGG Model Validation Loss')\n",
        "plt.title('Loss Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Analyze the plots\n",
        "print(\"Observations:\")\n",
        "if max(history.history['val_accuracy']) > max(vgg_history.history['val_accuracy']):\n",
        "    print(\"The basic model performs slightly better than the VGG model in terms of validation accuracy.\")\n",
        "elif max(vgg_history.history['val_accuracy']) > max(history.history['val_accuracy']):\n",
        "    print(\"The VGG model performs better than the basic model in terms of validation accuracy.\")\n",
        "else:\n",
        "    print(\"Both models have similar validation accuracy.\")\n",
        "\n",
        "if min(history.history['val_loss']) < min(vgg_history.history['val_loss']):\n",
        "    print(\"The basic model achieves a slightly lower validation loss than the VGG model.\")\n",
        "elif min(vgg_history.history['val_loss']) < min(history.history['val_loss']):\n",
        "    print(\"The VGG model achieves a lower validation loss than the basic model.\")\n",
        "else:\n",
        "    print(\"Both models have similar validation loss.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2cRr2ZFSFds"
      },
      "source": [
        "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8OSHAf5SJPr"
      },
      "outputs": [],
      "source": [
        "# Your code here :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri9kU3wa3Rei"
      },
      "source": [
        "**Comment on the observation**\n",
        "\n",
        "Observations:\n",
        "The VGG model performs better than the basic model in terms of validation accuracy.\n",
        "The VGG model achieves a lower validation loss than the basic model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzXmO1WoSKMY"
      },
      "source": [
        "*   Use predict function to predict the output for the test split\n",
        "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "DObaoxhaSMUg",
        "outputId": "1285e029-bbf3-4647-a88f-8f4cd2266ac2"
      },
      "outputs": [],
      "source": [
        "# prompt: Use predict function to predict the output for the test split\n",
        "# Plot the confusion matrix for the new model and comment on the class confusions.\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Predict the output for the test split using the VGG model\n",
        "y_pred = vgg_model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for VGG Model')\n",
        "plt.show()\n",
        "\n",
        "# Comment on the class confusions\n",
        "print(\"Confusion Matrix Analysis:\")\n",
        "# Analyze the confusion matrix to identify the most frequent misclassifications\n",
        "# Example:\n",
        "# Find classes with the highest number of misclassifications along the rows\n",
        "misclassifications_by_class = np.sum(conf_matrix, axis=1) - np.diag(conf_matrix)\n",
        "most_misclassified_class = np.argmax(misclassifications_by_class)\n",
        "print(f\"Class {most_misclassified_class} is most frequently misclassified.\")\n",
        "# Further detailed analysis can be done based on the specific values in the confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUBrvRomU5O_"
      },
      "source": [
        "**Comment here :**\n",
        "\n",
        "Confusion Matrix Analysis:\n",
        "Class 5 is most frequently misclassified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffwVz-FLSNG7"
      },
      "source": [
        "*    Print the test accuracy for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WX3_uLSN5I",
        "outputId": "62c7e524-e77b-45fa-a014-e705dcafe102"
      },
      "outputs": [],
      "source": [
        "# prompt: Print the test accuracy for the trained model.\n",
        "\n",
        "_, test_accuracy = vgg_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dySqfA6PVBjQ"
      },
      "source": [
        "## Define the complete VGG architecture.\n",
        "\n",
        "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer.\n",
        "\n",
        "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling.\n",
        "\n",
        "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "\n",
        "For all the layers, use ReLU activation function.\n",
        "\n",
        "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        "\n",
        "*   Change the size of input to 64 x 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm35siILFNT0"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session\n",
        "clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "oH4lDVBuVA_Q",
        "outputId": "146169c2-2cf4-41db-8fb6-8235427ffe8a"
      },
      "outputs": [],
      "source": [
        "# prompt: Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer.\n",
        "# Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling.\n",
        "# Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer.\n",
        "# For all the layers, use ReLU activation function.\n",
        "# Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
        "# Change the size of input to 64 x 64.\n",
        "\n",
        "clear_session()\n",
        "\n",
        "vgg_model = keras.Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "vgg_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu_B8kJGWhcM"
      },
      "source": [
        "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
        "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4elnDWnjEbmO",
        "outputId": "4a656333-7f31-4e65-c67a-4ab7ee86a56c"
      },
      "outputs": [],
      "source": [
        "# prompt: Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
        "# Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
        "# Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions.\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=512, validation_data=(x_test, y_test))\n",
        "\n",
        "# Predict the output for the test split\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for the Model')\n",
        "plt.show()\n",
        "\n",
        "# Comment on the class confusions (Example)\n",
        "print(\"Confusion Matrix Analysis:\")\n",
        "# Analyze the confusion matrix to identify the most frequent misclassifications\n",
        "# Example:\n",
        "# Find classes with the highest number of misclassifications along the rows\n",
        "misclassifications_by_class = np.sum(conf_matrix, axis=1) - np.diag(conf_matrix)\n",
        "most_misclassified_class = np.argmax(misclassifications_by_class)\n",
        "print(f\"Class {most_misclassified_class} is most frequently misclassified.\")\n",
        "# Further detailed analysis can be done based on the specific values in the confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dlzFt0SXGDQ"
      },
      "source": [
        "# Understanding deep networks\n",
        "\n",
        "*   What is the use of activation functions in network? Why is it needed?\n",
        "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
        "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXdsuI3lGzlC"
      },
      "source": [
        "# What is the use of activation functions in a network? Why is it needed?\n",
        "Activation functions introduce non-linearity into the network.  Without them, a neural network would simply be a series of linear transformations, which could be reduced to a single linear transformation.  Non-linearity is crucial for the network to learn complex patterns and relationships in data that can't be represented by a single linear function.  They enable the network to approximate highly complex functions.\n",
        "\n",
        "# We have used the softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
        "Sigmoid activation outputs a probability between 0 and 1 for *each* individual neuron.  It's commonly used in the output layer for binary classification problems.\n",
        " Softmax activation, on the other hand, outputs a probability distribution *over all neurons* in the output layer. The probabilities for all neurons sum up to 1.  This is used for multi-class classification where each neuron represents a different class.  Softmax allows the network to express the confidence of its prediction across multiple classes simultaneously.\n",
        "\n",
        "# What is the difference between categorical crossentropy and binary crossentropy loss?\n",
        "Binary cross-entropy is used when you have a binary classification problem (two classes). It measures the difference between the predicted probability distribution and the true probability distribution (0 or 1) for each data point.\n",
        "Categorical cross-entropy is used for multi-class classification problems.  It also measures the difference between the predicted probability distribution (output from softmax) and the true class label (one-hot encoded).  It's designed to handle multiple classes and inherently normalizes the output probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPy_1EWXX6fp"
      },
      "source": [
        "**Write the answers below :**\n",
        "\n",
        "1 - Use of activation functions:\n",
        "\n",
        "\n",
        "\n",
        "_\n",
        "\n",
        "2 - Key Differences between sigmoid and softmax:\n",
        "\n",
        "\n",
        "\n",
        "_\n",
        "\n",
        "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
        "\n",
        "\n",
        "_\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
