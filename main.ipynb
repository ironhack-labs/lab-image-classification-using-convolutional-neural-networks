{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4rCKcndPybL"
   },
   "source": [
    "# Lab : Image Classification using Convolutional Neural Networks\n",
    "\n",
    "At the end of this laboratory, you would get familiarized with\n",
    "\n",
    "*   Creating deep networks using Keras\n",
    "*   Steps necessary in training a neural network\n",
    "*   Prediction and performance analysis using neural networks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdglSzOi4Cp-"
   },
   "source": [
    "# **In case you use a colaboratory environment**\n",
    "By default, Colab notebooks run on CPU.\n",
    "You can switch your notebook to run with GPU.\n",
    "\n",
    "In order to obtain access to the GPU, you need to choose the tab Runtime and then select “Change runtime type” as shown in the following figure:\n",
    "\n",
    "![Changing runtime](https://miro.medium.com/max/747/1*euE7nGZ0uJQcgvkpgvkoQg.png)\n",
    "\n",
    "When a pop-up window appears select GPU. Ensure “Hardware accelerator” is set to GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wkicuxZdrdq"
   },
   "source": [
    "# **Working with a new dataset: CIFAR-10**\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. More information about CIFAR-10 can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "In Keras, the CIFAR-10 dataset is also preloaded in the form of four Numpy arrays. x_train and y_train contain the training set, while x_test and y_test contain the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9.\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "*   Visualize the images in CIFAR-10 dataset. Create a 10 x 10 plot showing 10 random samples from each class.\n",
    "*   Convert the labels to one-hot encoded form.\n",
    "*   Normalize the images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Mrb20KGMtTFq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Define class names for visualization\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Create a 10x10 plot showing 10 random samples from each class\n",
    "fig, axes = plt.subplots(10, 10, figsize=(15, 15))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "for i in range(10):\n",
    "    class_indices = np.where(y_train[:, 0] == i)[0]\n",
    "    random_indices = np.random.choice(class_indices, 10, replace=False)\n",
    "    for j, index in enumerate(random_indices):\n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(x_train[index])\n",
    "        ax.axis('off')\n",
    "        if j == 0:\n",
    "            ax.set_title(class_names[i])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Convert labels to one-hot encoded form\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Step 3: Normalize the images\n",
    "x_train_normalized = x_train.astype('float32') / 255.0\n",
    "x_test_normalized = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Print shapes to verify the preprocessing steps\n",
    "print(f\"x_train shape: {x_train_normalized.shape}\")\n",
    "print(f\"y_train shape: {y_train_one_hot.shape}\")\n",
    "print(f\"x_test shape: {x_test_normalized.shape}\")\n",
    "print(f\"y_test shape: {y_test_one_hot.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ER5WlMNRydp"
   },
   "source": [
    "## Define the following model (same as the one in tutorial)\n",
    "\n",
    "For the convolutional front-end, start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer. \n",
    "\n",
    "Use the input as (32,32,3). \n",
    "\n",
    "The filter maps can then be flattened to provide features to the classifier. \n",
    "\n",
    "Use a dense layer with 100 units before the classification layer (which is also a dense layer with softmax activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WfWCHxh8HGhN"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSN6riPISBMG"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a single convolutional layer\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "\n",
    "# Add a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a dense layer with 100 units\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "# Add the classification layer with softmax activation\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGtivbQJT39U"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn8UzPBZugVp"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# Train the model for 50 epochs with a batch size of 512\n",
    "history = model.fit(x_train_normalized, y_train_one_hot,\n",
    "                    batch_size=512,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_normalized, y_test_one_hot))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(x_test_normalized, y_test_one_hot)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('cifar10_model.h5')\n",
    "print(\"Model saved as cifar10_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Plot the cross entropy loss curve and the accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 7: Plot the cross-entropy loss curve and accuracy curve\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2mrWK5hSB_o"
   },
   "source": [
    "## Defining Deeper Architectures: VGG Models\n",
    "\n",
    "*   Define a deeper model architecture for CIFAR-10 dataset and train the new model for 50 epochs with a batch size of 512. We will use VGG model as the architecture.\n",
    "\n",
    "Stack two convolutional layers with 32 filters, each of 3 x 3. \n",
    "\n",
    "Use a max pooling layer and next flatten the output of the previous layer and add a dense layer with 128 units before the classification layer. \n",
    "\n",
    "For all the layers, use ReLU activation function. \n",
    "\n",
    "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "A80vLxW9FIek"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cgca5dUNSFNc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "# Step 1: Define the deeper VGG-inspired model\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.4))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Step 2: Compile the model\n",
    "\n",
    "model2.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwaPphEBUtlC"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 50 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc2qtU0mUvVA"
   },
   "outputs": [],
   "source": [
    "# Step 2: Compile the model\n",
    "optimizer = SGD()\n",
    "model2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "history2 = model2.fit(x_train_normalized, y_train_one_hot,\n",
    "                     batch_size=512,\n",
    "                     epochs=50,\n",
    "                     verbose=1,\n",
    "                     validation_data=(x_test_normalized, y_test_one_hot))\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model2.evaluate(x_test_normalized, y_test_one_hot)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2cRr2ZFSFds"
   },
   "source": [
    "*   Compare the performance of both the models by plotting the loss and accuracy curves of both the training steps. Does the deeper model perform better? Comment on the observation.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8OSHAf5SJPr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming history1 contains the training history for the initial model\n",
    "# and history2 contains the training history for the deeper VGG-inspired model\n",
    "\n",
    "# Plot training & validation loss values for both models\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Initial Model - Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Initial Model - Val Loss')\n",
    "plt.plot(history2.history['loss'], label='Deeper Model - Train Loss')\n",
    "plt.plot(history2.history['val_loss'], label='Deeper Model - Val Loss')\n",
    "plt.title('Model Loss Comparison')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Plot training & validation accuracy values for both models\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Initial Model - Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Initial Model - Val Accuracy')\n",
    "plt.plot(history2.history['accuracy'], label='Deeper Model - Train Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='Deeper Model - Val Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri9kU3wa3Rei"
   },
   "source": [
    "**Comment on the observation**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzXmO1WoSKMY"
   },
   "source": [
    "*   Use predict function to predict the output for the test split\n",
    "*   Plot the confusion matrix for the new model and comment on the class confusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DObaoxhaSMUg"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Step 5: Use the predict function to predict the output for the test split\n",
    "y_pred_prob = model2.predict(x_test_normalized)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Step 6: Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for VGG-inspired Model with Dropouts')\n",
    "plt.show()\n",
    "\n",
    "# Comment on class confusions\n",
    "print(\"Confusion Matrix Analysis:\")\n",
    "print(\"Each row represents the actual class, while each column represents the predicted class.\")\n",
    "print(\"Higher values off the diagonal indicate greater confusion between those classes.\")\n",
    "\n",
    "# Optionally, print the confusion matrix values\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUBrvRomU5O_"
   },
   "source": [
    "**Comment here :**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffwVz-FLSNG7"
   },
   "source": [
    "*    Print the test accuracy for the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4WX3_uLSN5I"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data and print the test accuracy\n",
    "test_loss, test_acc = model2.evaluate(x_test_normalized, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dySqfA6PVBjQ"
   },
   "source": [
    "## Define the complete VGG architecture.\n",
    "\n",
    "Stack two convolutional layers with 64 filters, each of 3 x 3 followed by max pooling layer. \n",
    "\n",
    "Stack two more convolutional layers with 128 filters, each of 3 x 3, followed by max pooling, followed by two more convolutional layers with 256 filters, each of 3 x 3, followed by max pooling. \n",
    "\n",
    "Flatten the output of the previous layer and add a dense layer with 128 units before the classification layer. \n",
    "\n",
    "For all the layers, use ReLU activation function. \n",
    "\n",
    "Use same padding for the layers to ensure that the height and width of each layer output matches the input\n",
    "\n",
    "*   Change the size of input to 64 x 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zm35siILFNT0"
   },
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH4lDVBuVA_Q"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Normalize the images and resize to 64x64\n",
    "x_train_resized = tf.image.resize(x_train, (64, 64)).numpy()\n",
    "x_test_resized = tf.image.resize(x_test, (64, 64)).numpy()\n",
    "x_train_normalized = x_train_resized.astype('float32') / 255.0\n",
    "x_test_normalized = x_test_resized.astype('float32') / 255.0\n",
    "\n",
    "# Step 4: Define the complete VGG architecture\n",
    "model_vgg = Sequential()\n",
    "# First stack of convolutional layers with 64 filters\n",
    "model_vgg.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
    "model_vgg.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model_vgg.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "# Second stack of convolutional layers with 128 filters\n",
    "model_vgg.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model_vgg.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model_vgg.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "# Third stack of convolutional layers with 256 filters\n",
    "model_vgg.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model_vgg.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model_vgg.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "# Flatten and add dense layers\n",
    "model_vgg.add(Flatten())\n",
    "model_vgg.add(Dense(128, activation='relu'))\n",
    "model_vgg.add(Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu_B8kJGWhcM"
   },
   "source": [
    "*   Compile the model using categorical_crossentropy loss, SGD optimizer and use 'accuracy' as the metric.\n",
    "*   Use the above defined model to train CIFAR-10 and train the model for 10 epochs with a batch size of 512.\n",
    "*   Predict the output for the test split and plot the confusion matrix for the new model and comment on the class confusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4elnDWnjEbmO"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_vgg.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "history_vgg = model_vgg.fit(x_train_normalized, y_train_one_hot,\n",
    "                            batch_size=512,\n",
    "                            epochs=10,\n",
    "                            verbose=1,\n",
    "                            validation_data=(x_test_normalized, y_test_one_hot))\n",
    "\n",
    "# Step 7: Use the predict function to predict the output for the test split\n",
    "y_pred_prob = model_vgg.predict(x_test_normalized)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Step 8: Plot the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "cmd.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for Complete VGG Model')\n",
    "plt.show()\n",
    "\n",
    "# Step 9: Print the test accuracy for the trained model\n",
    "test_loss, test_acc = model_vgg.evaluate(x_test_normalized, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")\n",
    "\n",
    "# Comment on class confusions\n",
    "print(\"Confusion Matrix Analysis:\")\n",
    "print(\"Each row represents the actual class, while each column represents the predicted class.\")\n",
    "print(\"Higher values off the diagonal indicate greater confusion between those classes.\")\n",
    "\n",
    "# Optionally, print the confusion matrix values\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dlzFt0SXGDQ"
   },
   "source": [
    "# Understanding deep networks\n",
    "\n",
    "*   What is the use of activation functions in network? Why is it needed?\n",
    "*   We have used softmax activation function in the exercise. There are other activation functions available too. What is the difference between sigmoid activation and softmax activation?\n",
    "*   What is the difference between categorical crossentropy and binary crossentropy loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPy_1EWXX6fp"
   },
   "source": [
    "**Write the answers below :**\n",
    "\n",
    "1 - Use of activation functions:\n",
    "\n",
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn and model complex relationships between inputs and outputs.\n",
    "_\n",
    "\n",
    "2 - Key Differences between sigmoid and softmax:\n",
    "\n",
    "Sigmoid Activation Function:\n",
    "\n",
    "Range: Outputs values between 0 and 1.\n",
    "Use Case: Commonly used for binary classification problems where the output represents the probability of a particular class.\n",
    "Independence: Each output is independent of the others.\n",
    "\n",
    "Softmax Activation Function:\n",
    "\n",
    "Range: Outputs a probability distribution that sums to 1 for a multi-class classification problem.\n",
    "Use Case: Typically used for multi-class classification problems where the output represents the probability of each class.\n",
    "Dependence: The output probabilities are interdependent and sum to 1.\n",
    "\n",
    "_\n",
    "\n",
    "3 - Key Differences between categorical crossentropy and binary crossentropy loss:\n",
    "\n",
    "Categorical Crossentropy Loss:\n",
    "\n",
    "Use Case: Used for multi-class classification problems where each sample belongs to one of many classes.\n",
    "\n",
    "Label Representation: Works with one-hot encoded labels.\n",
    "\n",
    "Calculation: Measures the difference between the predicted probability distribution and the actual distribution (one-hot encoded).\n",
    "\n",
    "Binary Crossentropy Loss:\n",
    "\n",
    "Use Case: Used for binary classification problems where each sample belongs to one of two classes.\n",
    "\n",
    "Label Representation: Works with binary labels (0 or 1).\n",
    "\n",
    "Calculation: Measures the difference between the predicted probability and the actual binary label.\n",
    "_\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
